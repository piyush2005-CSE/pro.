{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38dbe67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import pyttsx3\n",
    "import platform\n",
    "\n",
    "# --- 1. Initialization ---\n",
    "\n",
    "# Initialize Text-to-Speech Engine\n",
    "try:\n",
    "    engine = pyttsx3.init()\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize TTS engine: {e}\")\n",
    "    engine = None\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Cross-platform text-to-speech function.\"\"\"\n",
    "    if engine:\n",
    "        try:\n",
    "            # Stop any ongoing speech\n",
    "            engine.stop()\n",
    "            engine.say(text)\n",
    "            engine.runAndWait()\n",
    "        except Exception as e:\n",
    "            print(f\"TTS Error: {e}\")\n",
    "    # Fallback for macOS if pyttsx3 fails\n",
    "    elif platform.system() == 'Darwin':\n",
    "        try:\n",
    "            import os\n",
    "            os.system(f\"say '{text}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"macOS 'say' command failed: {e}\")\n",
    "    else:\n",
    "        print(f\"Speak: {text} (TTS not available)\")\n",
    "\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# --- 2. Gesture Recognition Logic ---\n",
    "\n",
    "# Gesture Definitions\n",
    "GESTURES = {\n",
    "    'VICTORY': 'Victory',\n",
    "    'THUMBS_UP': 'Thumbs Up',\n",
    "    'ONE': 'One',\n",
    "    'TWO': 'Two',\n",
    "    'THREE': 'Three',\n",
    "    'FOUR': 'Four',\n",
    "    'FIVE': 'Five / Open Hand',\n",
    "    'FIST': 'Fist / Closed Hand',\n",
    "}\n",
    "\n",
    "def recognize_gesture(landmarks):\n",
    "    \"\"\"Recognizes a gesture from hand landmarks.\"\"\"\n",
    "    if not landmarks:\n",
    "        return None\n",
    "\n",
    "    # Get coordinates for all landmarks\n",
    "    coords = np.array([(lm.x, lm.y, lm.z) for lm in landmarks.landmark])\n",
    "\n",
    "    # Finger landmark indices\n",
    "    finger_tips = [4, 8, 12, 16, 20]\n",
    "    finger_pips = [3, 6, 10, 14, 18] # PIP joints\n",
    "\n",
    "    # --- Thumbs Up Logic ---\n",
    "    thumb_tip = coords[4]\n",
    "    thumb_mcp = coords[2]\n",
    "    index_pip = coords[6]\n",
    "    middle_pip = coords[10]\n",
    "    \n",
    "    # Condition: Thumb tip is above its base and other fingertips are below their PIP joints.\n",
    "    is_thumb_up = thumb_tip[1] < thumb_mcp[1]\n",
    "    are_fingers_down = (\n",
    "        coords[8][1] > index_pip[1] and\n",
    "        coords[12][1] > middle_pip[1] and\n",
    "        coords[16][1] > coords[14][1] and\n",
    "        coords[20][1] > coords[18][1]\n",
    "    )\n",
    "    if is_thumb_up and are_fingers_down:\n",
    "        return GESTURES['THUMBS_UP']\n",
    "\n",
    "    # --- Finger Counting Logic ---\n",
    "    extended_fingers = 0\n",
    "    for i in range(5):\n",
    "        # A finger is extended if its tip is above its PIP joint.\n",
    "        # For thumb, check if tip is to the left/right of PIP (depending on hand)\n",
    "        if i == 0: # Thumb\n",
    "            # This logic checks horizontal extension for a flipped image\n",
    "            if coords[finger_tips[i]][0] < coords[finger_pips[i]][0]:\n",
    "                extended_fingers += 1\n",
    "        else:\n",
    "            if coords[finger_tips[i]][1] < coords[finger_pips[i]][1]:\n",
    "                extended_fingers += 1\n",
    "\n",
    "    # --- Map finger counts to gestures ---\n",
    "    if extended_fingers == 5: return GESTURES['FIVE']\n",
    "    if extended_fingers == 4: return GESTURES['FOUR']\n",
    "    if extended_fingers == 3: return GESTURES['THREE']\n",
    "    if extended_fingers == 2:\n",
    "        # Check for Victory sign (index and middle finger up)\n",
    "        index_up = coords[8][1] < coords[6][1]\n",
    "        middle_up = coords[12][1] < coords[10][1]\n",
    "        ring_down = coords[16][1] > coords[14][1]\n",
    "        if index_up and middle_up and ring_down:\n",
    "            return GESTURES['VICTORY']\n",
    "        return GESTURES['TWO']\n",
    "    if extended_fingers == 1: return GESTURES['ONE']\n",
    "    if extended_fingers == 0: return GESTURES['FIST']\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- 3. Main Application Loop ---\n",
    "\n",
    "# Stability and Speech Timing variables\n",
    "REQUIRED_STABILITY = 10\n",
    "MIN_SPEAK_DELAY = 2.0  # seconds\n",
    "\n",
    "stable_gesture = None\n",
    "stable_count = 0\n",
    "last_spoken_gesture = None\n",
    "last_spoken_time = 0\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open camera.\")\n",
    "else:\n",
    "    print(\"Camera started. Press 'ESC' to quit.\")\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        # Flip the frame horizontally for a later selfie-view display\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        frame.flags.setflags(write=False)\n",
    "        # Convert the BGR image to RGB.\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Process the frame and find hands\n",
    "        results = hands.process(rgb_frame)\n",
    "\n",
    "        # Allow writing to the frame again\n",
    "        frame.flags.setflags(write=True)\n",
    "\n",
    "        current_gesture = None\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                # Draw hand landmarks\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame,\n",
    "                    hand_landmarks,\n",
    "                    mp_hands.HAND_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),\n",
    "                    mp_drawing.DrawingSpec(color=(250, 44, 250), thickness=2, circle_radius=2),\n",
    "                )\n",
    "                # Recognize the gesture\n",
    "                current_gesture = recognize_gesture(hand_landmarks)\n",
    "\n",
    "        # --- Gesture Stability & Speech Logic ---\n",
    "        if current_gesture == stable_gesture:\n",
    "            stable_count += 1\n",
    "        else:\n",
    "            stable_gesture = current_gesture\n",
    "            stable_count = 0\n",
    "\n",
    "        now = time.time()\n",
    "        if (\n",
    "            stable_count > REQUIRED_STABILITY and\n",
    "            current_gesture is not None and\n",
    "            current_gesture != last_spoken_gesture and\n",
    "            (now - last_spoken_time) > MIN_SPEAK_DELAY\n",
    "        ):\n",
    "            print(f\"Gesture Detected: {current_gesture}\")\n",
    "            speak(current_gesture)\n",
    "            last_spoken_gesture = current_gesture\n",
    "            last_spoken_time = now\n",
    "            stable_count = 0  # Reset after speaking\n",
    "\n",
    "        # --- Display Information on Frame ---\n",
    "        # Status box\n",
    "        cv2.rectangle(frame, (0, 0), (300, 80), (20, 20, 20), -1)\n",
    "        # Detected Gesture Text\n",
    "        display_gesture = stable_gesture if stable_gesture else \"None\"\n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Gesture: {display_gesture}\",\n",
    "            (10, 60),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            1,\n",
    "            (255, 255, 255),\n",
    "            2,\n",
    "            cv2.LINE_AA\n",
    "        )\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Hand Gesture to Speech', frame)\n",
    "\n",
    "        # Exit on 'ESC' key\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "# --- 4. Cleanup ---\n",
    "print(\"Cleaning up...\")\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "# Ensure the TTS engine shuts down cleanly\n",
    "if engine:\n",
    "    engine.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c42af9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import pyttsx3\n",
    "import platform\n",
    "from collections import deque\n",
    "\n",
    "# --- 1. Initialization ---\n",
    "\n",
    "# Initialize Text-to-Speech Engine\n",
    "try:\n",
    "    engine = pyttsx3.init()\n",
    "except Exception as e:\n",
    "    print(f\"Could not initialize TTS engine: {e}\")\n",
    "    engine = None\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Cross-platform text-to-speech function.\"\"\"\n",
    "    if engine:\n",
    "        try:\n",
    "            engine.stop()\n",
    "            engine.say(text)\n",
    "            engine.runAndWait()\n",
    "        except Exception as e:\n",
    "            print(f\"TTS Error: {e}\")\n",
    "    elif platform.system() == 'Darwin':\n",
    "        try:\n",
    "            import os\n",
    "            os.system(f\"say '{text}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"macOS 'say' command failed: {e}\")\n",
    "    else:\n",
    "        print(f\"Speak: {text} (TTS not available)\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(\n",
    "    max_num_hands=1,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# --- 2. Enhanced Gesture Recognition Logic ---\n",
    "\n",
    "# Gesture Definitions\n",
    "GESTURES = {\n",
    "    'VICTORY': 'Victory',\n",
    "    'THUMBS_UP': 'Thumbs Up',\n",
    "    'ONE': 'One',\n",
    "    'TWO': 'Two',\n",
    "    'THREE': 'Three',\n",
    "    'FOUR': 'Four',\n",
    "    'FIVE': 'Five / Open Hand',\n",
    "    'FIST': 'Fist / Closed Hand',\n",
    "    'OK': 'OK',\n",
    "}\n",
    "\n",
    "# This list will be populated by the recognition function to give visual feedback\n",
    "finger_status = {'extended': [], 'closed': []}\n",
    "\n",
    "def recognize_gesture(landmarks, hand_size):\n",
    "    \"\"\"\n",
    "    Recognizes a gesture from hand landmarks using normalized distances.\n",
    "    Returns the recognized gesture and updates the global finger_status.\n",
    "    \"\"\"\n",
    "    global finger_status\n",
    "    finger_status = {'extended': [], 'closed': []}\n",
    "    \n",
    "    if not landmarks or hand_size == 0:\n",
    "        return None\n",
    "\n",
    "    # Get coordinates for all landmarks\n",
    "    coords = np.array([(lm.x, lm.y, lm.z) for lm in landmarks.landmark])\n",
    "    \n",
    "    # --- Finger Extension Calculation ---\n",
    "    # We determine if a finger is extended by checking the vertical distance \n",
    "    # between the tip and the MCP joint, normalized by hand size.\n",
    "    \n",
    "    finger_tips_indices = [4, 8, 12, 16, 20]\n",
    "    finger_mcp_indices = [2, 5, 9, 13, 17]\n",
    "    \n",
    "    extended_fingers = []\n",
    "    for i, (tip_idx, mcp_idx) in enumerate(zip(finger_tips_indices, finger_mcp_indices)):\n",
    "        tip = coords[tip_idx]\n",
    "        mcp = coords[mcp_idx]\n",
    "        # For thumb, we check horizontal distance from wrist\n",
    "        if i == 0: \n",
    "            wrist = coords[0]\n",
    "            # Use a threshold based on hand orientation (assuming right hand for logic)\n",
    "            # A more robust check might use dot products to find true extension direction.\n",
    "            is_extended = (abs(tip[0] - mcp[0])) > 0.04 \n",
    "        else:\n",
    "            is_extended = (mcp[1] - tip[1]) / hand_size > 0.15\n",
    "\n",
    "        if is_extended:\n",
    "            extended_fingers.append(i)\n",
    "            finger_status['extended'].append(tip_idx)\n",
    "        else:\n",
    "            finger_status['closed'].append(tip_idx)\n",
    "\n",
    "    num_extended = len(extended_fingers)\n",
    "\n",
    "    # --- Gesture Mapping ---\n",
    "    \n",
    "    # OK Gesture: Thumb and Index finger tips are close, other fingers extended.\n",
    "    thumb_tip = coords[4]\n",
    "    index_tip = coords[8]\n",
    "    tip_distance = np.linalg.norm(thumb_tip - index_tip) / hand_size\n",
    "    if tip_distance < 0.08 and all(f in extended_fingers for f in [2, 3, 4]):\n",
    "        return GESTURES['OK']\n",
    "\n",
    "    # Thumbs Up: Only thumb is extended.\n",
    "    if num_extended == 1 and 0 in extended_fingers:\n",
    "        return GESTURES['THUMBS_UP']\n",
    "        \n",
    "    # Victory: Index and Middle fingers extended.\n",
    "    if num_extended == 2 and 1 in extended_fingers and 2 in extended_fingers:\n",
    "        return GESTURES['VICTORY']\n",
    "\n",
    "    # Map remaining finger counts to gestures\n",
    "    if num_extended == 5: return GESTURES['FIVE']\n",
    "    if num_extended == 4: return GESTURES['FOUR']\n",
    "    if num_extended == 3: return GESTURES['THREE']\n",
    "    if num_extended == 2: return GESTURES['TWO']\n",
    "    if num_extended == 1: return GESTURES['ONE']\n",
    "    if num_extended == 0: return GESTURES['FIST']\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- 3. UI and Drawing Functions ---\n",
    "\n",
    "def draw_ui(frame, gesture, history):\n",
    "    \"\"\"Draws the main UI elements on the frame.\"\"\"\n",
    "    # Status box\n",
    "    cv2.rectangle(frame, (0, 0), (400, 80), (20, 20, 20), -1)\n",
    "    display_gesture = gesture if gesture else \"None\"\n",
    "    cv2.putText(frame, f\"Gesture: {display_gesture}\", (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Gesture History Box\n",
    "    history_x = frame.shape[1] - 280\n",
    "    cv2.rectangle(frame, (history_x, 0), (frame.shape[1], 180), (20, 20, 20), -1)\n",
    "    cv2.putText(frame, \"History:\", (history_x + 10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    for i, old_gesture in enumerate(history):\n",
    "        cv2.putText(frame, old_gesture, (history_x + 10, 60 + i * 25),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1, cv2.LINE_AA)\n",
    "\n",
    "def draw_finger_feedback(frame, landmarks, status):\n",
    "    \"\"\"Draws green/red circles on fingertips for feedback.\"\"\"\n",
    "    if not landmarks: return\n",
    "    \n",
    "    h, w, _ = frame.shape\n",
    "    coords = np.array([(lm.x, lm.y) for lm in landmarks.landmark])\n",
    "    coords = (coords * [w, h]).astype(int)\n",
    "\n",
    "    for idx in status['extended']:\n",
    "        cv2.circle(frame, tuple(coords[idx]), 10, (0, 255, 0), -1) # Green for extended\n",
    "    for idx in status['closed']:\n",
    "        cv2.circle(frame, tuple(coords[idx]), 10, (0, 0, 255), -1) # Red for closed\n",
    "\n",
    "# --- 4. Main Application Loop ---\n",
    "\n",
    "def run_gesture_recognition():\n",
    "    \"\"\"Main function to run the camera, calibration, and recognition loop.\"\"\"\n",
    "    # Stability and Speech Timing variables\n",
    "    REQUIRED_STABILITY = 10\n",
    "    MIN_SPEAK_DELAY = 2.0  # seconds\n",
    "\n",
    "    stable_gesture = None\n",
    "    stable_count = 0\n",
    "    last_spoken_gesture = None\n",
    "    last_spoken_time = 0\n",
    "    \n",
    "    gesture_history = deque(maxlen=5)\n",
    "    hand_size = 0  # Will be set during calibration\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open camera.\")\n",
    "        return\n",
    "\n",
    "    # --- Calibration Phase ---\n",
    "    calibration_start_time = time.time()\n",
    "    calibration_duration = 3.0\n",
    "    calibrated = False\n",
    "    \n",
    "    print(\"Starting calibration... Hold your hand open in front of the camera.\")\n",
    "\n",
    "    while time.time() - calibration_start_time < calibration_duration:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: continue\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        \n",
    "        # Display calibration message\n",
    "        time_left = calibration_duration - (time.time() - calibration_start_time)\n",
    "        cv2.putText(frame, f\"CALIBRATING... {time_left:.1f}s\", (50, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, \"Hold your hand open\", (50, 100), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(rgb_frame)\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            # Calculate hand size as distance from wrist to middle finger MCP\n",
    "            wrist = hand_landmarks.landmark[0]\n",
    "            middle_mcp = hand_landmarks.landmark[9]\n",
    "            hand_size = np.linalg.norm([wrist.x - middle_mcp.x, wrist.y - middle_mcp.y])\n",
    "            calibrated = True\n",
    "\n",
    "        cv2.imshow('Hand Gesture to Speech', frame)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            return\n",
    "\n",
    "    if not calibrated:\n",
    "        print(\"Calibration failed. Could not detect hand. Using default values.\")\n",
    "        hand_size = 0.2 # Fallback value\n",
    "\n",
    "    print(\"Calibration Complete! Starting main application.\")\n",
    "    speak(\"Calibration Complete\")\n",
    "    \n",
    "    # --- Main Loop ---\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: continue\n",
    "\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb_frame.flags.setflags(write=False)\n",
    "        results = hands.process(rgb_frame)\n",
    "        rgb_frame.flags.setflags(write=True)\n",
    "\n",
    "        current_gesture = None\n",
    "        if results.multi_hand_landmarks:\n",
    "            hand_landmarks = results.multi_hand_landmarks[0]\n",
    "            current_gesture = recognize_gesture(hand_landmarks, hand_size)\n",
    "            \n",
    "            # Draw landmarks and finger feedback\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,\n",
    "                mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "                mp_drawing_styles.get_default_hand_connections_style())\n",
    "            draw_finger_feedback(frame, hand_landmarks, finger_status)\n",
    "\n",
    "        # Update gesture stability\n",
    "        if current_gesture == stable_gesture:\n",
    "            stable_count += 1\n",
    "        else:\n",
    "            stable_gesture = current_gesture\n",
    "            stable_count = 0\n",
    "\n",
    "        # Speak gesture if stable\n",
    "        now = time.time()\n",
    "        if (stable_count > REQUIRED_STABILITY and current_gesture is not None and\n",
    "            current_gesture != last_spoken_gesture and (now - last_spoken_time) > MIN_SPEAK_DELAY):\n",
    "            \n",
    "            print(f\"Gesture Detected: {current_gesture}\")\n",
    "            speak(current_gesture)\n",
    "            last_spoken_gesture = current_gesture\n",
    "            last_spoken_time = now\n",
    "            if current_gesture not in gesture_history:\n",
    "                gesture_history.appendleft(current_gesture)\n",
    "            stable_count = 0\n",
    "\n",
    "        # Draw UI elements\n",
    "        draw_ui(frame, stable_gesture, gesture_history)\n",
    "        \n",
    "        cv2.imshow('Hand Gesture to Speech', frame)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    # --- Cleanup ---\n",
    "    print(\"Cleaning up...\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    if engine:\n",
    "        engine.stop()\n",
    "\n",
    "# --- 5. Run the Application ---\n",
    "if __name__ == '__main__':\n",
    "    # In a notebook, you can just call the function directly.\n",
    "    # The if __name__ == '__main__': is good practice for Python scripts.\n",
    "    run_gesture_recognition()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
